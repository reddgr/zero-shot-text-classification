{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot text classification with Natural Language Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
      "PyTorch version: 2.2.2\n",
      "Transformers version: 4.44.2\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA Version: 12.1\n",
      "FlashAttention available: True\n",
      "Retrieved token(s) from .env file\n",
      "Using HuggingFace token: hf_M*****************************IASJ\n",
      "Using HuggingFace write token: hf_u*****************************Xipx\n",
      "Using OpenAI token: sk-p************************************************************************************************************************************************************_5sA\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import env_options\n",
    "import nli_finetuning_utils\n",
    "import lmsys_dataset_handler as lmsys\n",
    "import text_classification_functions as tcf\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import textwrap\n",
    "from IPython.display import clear_output\n",
    "hf_token, hf_token_write, openai_api_key = env_options.check_env(dotenv_path='../../../../../../apis/.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Testing inference with pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 AG News dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 7600 records. Sample\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Man Sought  #36;50M From McGreevey, Aides Say ...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>Hosted E-Mail Service Leaves Windows for Linux...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>UPI NewsTrack Sports -- The United States men ...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5137</th>\n",
       "      <td>Retail, auto sales, job numbers suggest toughe...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>This Just In - Sprint is Stupid \\\\Found this  ...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     class\n",
       "33    Man Sought  #36;50M From McGreevey, Aides Say ...     World\n",
       "4869  Hosted E-Mail Service Leaves Windows for Linux...  Sci/Tech\n",
       "128   UPI NewsTrack Sports -- The United States men ...    Sports\n",
       "5137  Retail, auto sales, job numbers suggest toughe...  Business\n",
       "4498  This Just In - Sprint is Stupid \\\\Found this  ...  Sci/Tech"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"fancyzhx/ag_news\", split=\"test\")\n",
    "use_sampled_dataset=False\n",
    "if use_sampled_dataset:\n",
    "    dataset = dataset.shuffle(seed=42).select(range(100))\n",
    "agn_labels = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "dataset = dataset.map(lambda x: {\"class\": agn_labels[x[\"label\"]]}, remove_columns=[\"label\"])\n",
    "df_agnews = dataset.to_pandas()\n",
    "print(f\"Extracted {len(df_agnews)} records. Sample\")\n",
    "display(df_agnews.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing inference with facebook/bart-large-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nli_model_path = 'facebook/bart-large-mnli'\n",
    "zs_classifier_agnews = tcf.ZeroShotClassifier(model_path=nli_model_path, tokenizer_path=nli_model_path, candidate_labels=agn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:496: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'AUBURN 21, ALABAMA 13 Auburn #39;s Strong Second Half Keeps It in &lt;b&gt;...&lt;/b&gt; For one half Saturday, the controversy over the Bowl Championship Series looked like it might disappear in the dampness of Bryant-Denny Stadium as undefeated Auburn found itself in a fight with archrival Alabama.',\n",
       " 'labels': ['Sports', 'World', 'Sci/Tech', 'Business'],\n",
       " 'scores': [0.798, 0.09, 0.064, 0.048]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sample = df_agnews.sample(1).text.values[0]\n",
    "zs_classifier_agnews.classify_text(text_sample, multi_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing inference with reddgr/zero-shot-prompt-classifier-bart-ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f2fcebcf7f4740bde6ae2cb414a09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\david\\.cache\\huggingface\\hub\\models--reddgr--zero-shot-prompt-classifier-bart-ft. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b392667c7e40fd9adf2ebf32f5bbf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc61ffa5aa34bae81b55576350c250c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35cf89482d945b08b1b087672fb7820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b14f53f0fed4a1696299e482d1533ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5413e23d17f84a25b897feb9840dd288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912d9e42e50446e09923c1887fccc194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/294 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'AUBURN 21, ALABAMA 13 Auburn #39;s Strong Second Half Keeps It in &lt;b&gt;...&lt;/b&gt; For one half Saturday, the controversy over the Bowl Championship Series looked like it might disappear in the dampness of Bryant-Denny Stadium as undefeated Auburn found itself in a fight with archrival Alabama.',\n",
       " 'labels': ['Sports', 'Business', 'World', 'Sci/Tech'],\n",
       " 'scores': [0.851, 0.062, 0.057, 0.029]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_model_path_r = 'reddgr/zero-shot-prompt-classifier-bart-ft'\n",
    "zs_classifier_agnews_r = tcf.ZeroShotClassifier(model_path=nli_model_path_r, tokenizer_path=nli_model_path_r, candidate_labels=agn_labels)\n",
    "# Classifying the same text sample:\n",
    "zs_classifier_agnews_r.classify_text(text_sample, multi_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.74it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>top_class_zs</th>\n",
       "      <th>top_score_zs</th>\n",
       "      <th>full_results_zs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6182</th>\n",
       "      <td>Westwood Closes in on First Title of 2004  SUN...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0.431</td>\n",
       "      <td>[(Sports, 0.431), (World, 0.304), (Sci/Tech, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7146</th>\n",
       "      <td>HP targets China with low-cost PC SAN FRANCISC...</td>\n",
       "      <td>Business</td>\n",
       "      <td>World</td>\n",
       "      <td>0.532</td>\n",
       "      <td>[(World, 0.532), (Sci/Tech, 0.224), (Business,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>October Games Provide Moments to Remember FOR ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0.969</td>\n",
       "      <td>[(Sports, 0.969), (World, 0.018), (Business, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>Police Tear Gas, Arrest Protesters in Banglade...</td>\n",
       "      <td>World</td>\n",
       "      <td>World</td>\n",
       "      <td>0.437</td>\n",
       "      <td>[(World, 0.437), (Sci/Tech, 0.362), (Sports, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>Civil servants in net porn probe More than 200...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>World</td>\n",
       "      <td>0.489</td>\n",
       "      <td>[(World, 0.489), (Sci/Tech, 0.308), (Business,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>Myanmar frees nearly 4,000 prisoners UN Secret...</td>\n",
       "      <td>World</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>0.397</td>\n",
       "      <td>[(Sci/Tech, 0.397), (World, 0.267), (Sports, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>Tributes pour in for  #39;Crazy Horse #39; Hug...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>World</td>\n",
       "      <td>0.469</td>\n",
       "      <td>[(World, 0.469), (Sci/Tech, 0.283), (Business,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2790</th>\n",
       "      <td>Salesforce.com launches on-demand support com ...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Business</td>\n",
       "      <td>0.438</td>\n",
       "      <td>[(Business, 0.438), (Sci/Tech, 0.308), (World,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>A Fair Tax Some say a \"fair tax\" that removes ...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>World</td>\n",
       "      <td>0.499</td>\n",
       "      <td>[(World, 0.499), (Sci/Tech, 0.235), (Business,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Phelps Eyes Fourth Gold  ATHENS (Reuters) - A ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0.738</td>\n",
       "      <td>[(Sports, 0.738), (World, 0.137), (Sci/Tech, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     class  \\\n",
       "6182  Westwood Closes in on First Title of 2004  SUN...    Sports   \n",
       "7146  HP targets China with low-cost PC SAN FRANCISC...  Business   \n",
       "4199  October Games Provide Moments to Remember FOR ...    Sports   \n",
       "552   Police Tear Gas, Arrest Protesters in Banglade...     World   \n",
       "623   Civil servants in net porn probe More than 200...  Sci/Tech   \n",
       "6143  Myanmar frees nearly 4,000 prisoners UN Secret...     World   \n",
       "5449  Tributes pour in for  #39;Crazy Horse #39; Hug...    Sports   \n",
       "2790  Salesforce.com launches on-demand support com ...  Sci/Tech   \n",
       "6068  A Fair Tax Some say a \"fair tax\" that removes ...  Sci/Tech   \n",
       "158   Phelps Eyes Fourth Gold  ATHENS (Reuters) - A ...    Sports   \n",
       "\n",
       "     top_class_zs  top_score_zs  \\\n",
       "6182       Sports         0.431   \n",
       "7146        World         0.532   \n",
       "4199       Sports         0.969   \n",
       "552         World         0.437   \n",
       "623         World         0.489   \n",
       "6143     Sci/Tech         0.397   \n",
       "5449        World         0.469   \n",
       "2790     Business         0.438   \n",
       "6068        World         0.499   \n",
       "158        Sports         0.738   \n",
       "\n",
       "                                        full_results_zs  \n",
       "6182  [(Sports, 0.431), (World, 0.304), (Sci/Tech, 0...  \n",
       "7146  [(World, 0.532), (Sci/Tech, 0.224), (Business,...  \n",
       "4199  [(Sports, 0.969), (World, 0.018), (Business, 0...  \n",
       "552   [(World, 0.437), (Sci/Tech, 0.362), (Sports, 0...  \n",
       "623   [(World, 0.489), (Sci/Tech, 0.308), (Business,...  \n",
       "6143  [(Sci/Tech, 0.397), (World, 0.267), (Sports, 0...  \n",
       "5449  [(World, 0.469), (Sci/Tech, 0.283), (Business,...  \n",
       "2790  [(Business, 0.438), (Sci/Tech, 0.308), (World,...  \n",
       "6068  [(World, 0.499), (Sci/Tech, 0.235), (Business,...  \n",
       "158   [(Sports, 0.738), (World, 0.137), (Sci/Tech, 0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_testing = df_agnews.sample(10).copy()\n",
    "df_testing_zs = zs_classifier_agnews.classify_dataframe_column(df_testing, target_column = 'text', feature_suffix = 'zs') \n",
    "display(df_testing_zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 LMSYS Chatbot Arena data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from train-00002-of-00006-1779b7cec9462180.parquet\n",
      "Retrieved 1000 random conversations from lmsys/lmsys-chat-1m/train-00002-of-00006-1779b7cec9462180.parquet\n",
      "Extracted data from lmsys/lmsys-chat-1m. Prompt sample:\n",
      "\n",
      "Hello, How are you?\n"
     ]
    }
   ],
   "source": [
    "lmsys_chat_1m = lmsys.LMSYSChat1MHandler(hf_token, streaming=False, verbose=False)\n",
    "df_sample = lmsys_chat_1m.parquet_sampling(1000) # the method parquet_sampling() selects the samples from a random parquet file so it doesn't download the whole dataset\n",
    "df_prompts = lmsys_chat_1m.extract_prompts(filter_language=['English', 'Spanish'], max_char_length=400)\n",
    "prompt_sample = lmsys_chat_1m.extract_prompt_sample()\n",
    "print(\"Extracted data from lmsys/lmsys-chat-1m. Prompt sample:\\n\")\n",
    "print(prompt_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>model</th>\n",
       "      <th>conversation</th>\n",
       "      <th>turn</th>\n",
       "      <th>language</th>\n",
       "      <th>openai_moderation</th>\n",
       "      <th>redacted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46923</th>\n",
       "      <td>e5bfdb9899be47b4977b01bfd61814ff</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>[{'content': 'write an eassy with helpful reso...</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162103</th>\n",
       "      <td>6053795aadb24782ab4db1930586f2a5</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>[{'content': 'Present a paradox that Is not co...</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124129</th>\n",
       "      <td>4ab9192210854e49aa06d014ba0fee1a</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>[{'content': 'You are the text completion mode...</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160402</th>\n",
       "      <td>ac1c5e3af53f4cfa80b2e470ab1500b2</td>\n",
       "      <td>wizardlm-13b</td>\n",
       "      <td>[{'content': 'Who was the physically strongest...</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133516</th>\n",
       "      <td>13bb1ee1a6fb4fbb9d006ead54d1dd8c</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>[{'content': 'Translate English to SQL.\n",
       "Englis...</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65035</th>\n",
       "      <td>3f24471a3a8246bcbc53f7b45a2b8c83</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>[{'content': 'can you share the brc20 with me'...</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         conversation_id         model  \\\n",
       "46923   e5bfdb9899be47b4977b01bfd61814ff    vicuna-13b   \n",
       "162103  6053795aadb24782ab4db1930586f2a5    vicuna-13b   \n",
       "124129  4ab9192210854e49aa06d014ba0fee1a    vicuna-13b   \n",
       "160402  ac1c5e3af53f4cfa80b2e470ab1500b2  wizardlm-13b   \n",
       "133516  13bb1ee1a6fb4fbb9d006ead54d1dd8c    vicuna-13b   \n",
       "65035   3f24471a3a8246bcbc53f7b45a2b8c83    vicuna-13b   \n",
       "\n",
       "                                             conversation  turn language  \\\n",
       "46923   [{'content': 'write an eassy with helpful reso...     1  English   \n",
       "162103  [{'content': 'Present a paradox that Is not co...     1  English   \n",
       "124129  [{'content': 'You are the text completion mode...     1  English   \n",
       "160402  [{'content': 'Who was the physically strongest...     1  English   \n",
       "133516  [{'content': 'Translate English to SQL.\n",
       "Englis...     1  English   \n",
       "65035   [{'content': 'can you share the brc20 with me'...     1  English   \n",
       "\n",
       "                                        openai_moderation  redacted  \n",
       "46923   [{'categories': {'harassment': False, 'harassm...     False  \n",
       "162103  [{'categories': {'harassment': False, 'harassm...     False  \n",
       "124129  [{'categories': {'harassment': False, 'harassm...     False  \n",
       "160402  [{'categories': {'harassment': False, 'harassm...     False  \n",
       "133516  [{'categories': {'harassment': False, 'harassm...     False  \n",
       "65035   [{'categories': {'harassment': False, 'harassm...      True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation ID 6053795aadb24782ab4db1930586f2a5:\n",
      "\n",
      "[{'content': 'Present a paradox that Is not commonly known', 'role': 'user', 'turn': 1}\n",
      " {'content': 'The paradox of the stone is a lesser-known paradox that raises questions about the nature of predication and reference. It is a form of the liar paradox, which is a family of paradoxes that arise from attempts to define a predicate in circular or self-referential ways.\\n\\nThe paradox of the stone is as follows:\\n\\n1. A stone raises itself above itself.\\n2. This statement is not true.\\n3. Therefore, the stone cannot raise itself above itself.\\n\\nThe paradox arises because the statement \"this statement is not true\" appears to refer to itself, but it cannot do so without violating the principle that a predicate must refer to something in the domain of discourse. If the statement refers to itself, then it is true, which leads to a circular reasoning. On the other hand, if the statement refers to something outside of itself, then it is not true, which leads to a self-referential paradox.\\n\\nThe paradox of the stone highlights the difficulty of defining predicates in a way that avoids self-referentiality and circular reasoning. It also raises questions about the nature of truth and reference, and the limits of language in expressing reality.', 'role': 'assistant', 'turn': 1}]\n"
     ]
    }
   ],
   "source": [
    "df_sample = lmsys_chat_1m.add_turns_to_conversations()\n",
    "display(df_sample.head(6))\n",
    "print(f\"Conversation ID {df_sample.iloc[1]['conversation_id']}:\\n\")\n",
    "print(df_sample['conversation'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>model</th>\n",
       "      <th>conversation_turns</th>\n",
       "      <th>language</th>\n",
       "      <th>openai_moderation</th>\n",
       "      <th>redacted</th>\n",
       "      <th>turn_n</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e5bfdb9899be47b4977b01bfd61814ff</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>write an eassy with helpful resources and exam...</td>\n",
       "      <td>Effective communication with teenagers is esse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6053795aadb24782ab4db1930586f2a5</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Present a paradox that Is not commonly known</td>\n",
       "      <td>The paradox of the stone is a lesser-known par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4ab9192210854e49aa06d014ba0fee1a</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>You are the text completion model and you must...</td>\n",
       "      <td>To disable the `help` command in Discord.py, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ac1c5e3af53f4cfa80b2e470ab1500b2</td>\n",
       "      <td>wizardlm-13b</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Who was the physically strongest member of the...</td>\n",
       "      <td>The physically strongest member of the Legion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13bb1ee1a6fb4fbb9d006ead54d1dd8c</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Translate English to SQL.\\nEnglish: Find the f...</td>\n",
       "      <td>The SQL query to find the frame id with a dog ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3f24471a3a8246bcbc53f7b45a2b8c83</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>can you share the brc20 with me</td>\n",
       "      <td>Sure, here is the BRCP20 checklist that is use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6a44e25ce6bd45c8ae8f92c73d1ac11a</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Write an article about the Instruction of 2-AM...</td>\n",
       "      <td>The instruction of 2-aminothiophenol (2-ATP) 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0b47d782259f4a49801cae731bf18767</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "      <td>[{'categories': {'harassment': False, 'harassm...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>I am doing a case study on the company SpiceJe...</td>\n",
       "      <td>I will provide a competitor analysis of SpiceJ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    conversation_id         model  conversation_turns  \\\n",
       "0  e5bfdb9899be47b4977b01bfd61814ff    vicuna-13b                   1   \n",
       "1  6053795aadb24782ab4db1930586f2a5    vicuna-13b                   1   \n",
       "2  4ab9192210854e49aa06d014ba0fee1a    vicuna-13b                   1   \n",
       "3  ac1c5e3af53f4cfa80b2e470ab1500b2  wizardlm-13b                   1   \n",
       "4  13bb1ee1a6fb4fbb9d006ead54d1dd8c    vicuna-13b                   1   \n",
       "5  3f24471a3a8246bcbc53f7b45a2b8c83    vicuna-13b                   1   \n",
       "6  6a44e25ce6bd45c8ae8f92c73d1ac11a     koala-13b                   1   \n",
       "7  0b47d782259f4a49801cae731bf18767    alpaca-13b                   1   \n",
       "\n",
       "  language                                  openai_moderation  redacted  \\\n",
       "0  English  [{'categories': {'harassment': False, 'harassm...     False   \n",
       "1  English  [{'categories': {'harassment': False, 'harassm...     False   \n",
       "2  English  [{'categories': {'harassment': False, 'harassm...     False   \n",
       "3  English  [{'categories': {'harassment': False, 'harassm...     False   \n",
       "4  English  [{'categories': {'harassment': False, 'harassm...     False   \n",
       "5  English  [{'categories': {'harassment': False, 'harassm...      True   \n",
       "6  English  [{'categories': {'harassment': False, 'harassm...     False   \n",
       "7  English  [{'categories': {'harassment': False, 'harassm...     False   \n",
       "\n",
       "   turn_n                                             prompt  \\\n",
       "0       1  write an eassy with helpful resources and exam...   \n",
       "1       1       Present a paradox that Is not commonly known   \n",
       "2       1  You are the text completion model and you must...   \n",
       "3       1  Who was the physically strongest member of the...   \n",
       "4       1  Translate English to SQL.\\nEnglish: Find the f...   \n",
       "5       1                    can you share the brc20 with me   \n",
       "6       1  Write an article about the Instruction of 2-AM...   \n",
       "7       1  I am doing a case study on the company SpiceJe...   \n",
       "\n",
       "                                            response  \n",
       "0  Effective communication with teenagers is esse...  \n",
       "1  The paradox of the stone is a lesser-known par...  \n",
       "2  To disable the `help` command in Discord.py, y...  \n",
       "3  The physically strongest member of the Legion ...  \n",
       "4  The SQL query to find the frame id with a dog ...  \n",
       "5  Sure, here is the BRCP20 checklist that is use...  \n",
       "6  The instruction of 2-aminothiophenol (2-ATP) 1...  \n",
       "7  I will provide a competitor analysis of SpiceJ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_unwrapped_turns = lmsys_chat_1m.unwrap_turns()\n",
    "display(df_unwrapped_turns.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt categories are not very efficiently inferred by a model trained on other types of texts (news, articles, human chats...). We will require some finetuning, but we can see how pretrained facebook/bart-large-mnli gives some reasonable \"zero-shot\" outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nli_model_path = 'facebook/bart-large-mnli'\n",
    "labels = [\"Code\", \"Language\", \"Sci/Tech\", \"Business\", \"Q&A\", \"Role play\"] \n",
    "zs_classifier = tcf.ZeroShotClassifier(nli_model_path, nli_model_path, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Can you make something like\\n\"A\": Going to a friend\\'s baby shower today, \"B\": Wrapping a gift\\n\"A\": Trying a new vegan recipe for dinner tonight, \"B\": Chopping vegetables',\n",
       " 'labels': ['Q&A', 'Language', 'Role play', 'Sci/Tech', 'Code', 'Business'],\n",
       " 'scores': [0.556, 0.21, 0.109, 0.05, 0.049, 0.027]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_sample = lmsys_chat_1m.extract_prompt_sample()\n",
    "zs_classifier.classify_text(prompt_sample, multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>language</th>\n",
       "      <th>top_class_zs</th>\n",
       "      <th>top_score_zs</th>\n",
       "      <th>full_results_zs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>Predict a quote from the Chuck E. Cheese final...</td>\n",
       "      <td>English</td>\n",
       "      <td>Language</td>\n",
       "      <td>0.289</td>\n",
       "      <td>[(Language, 0.289), (Role play, 0.256), (Code,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>Explain the joke: NAME_1 and his father were c...</td>\n",
       "      <td>English</td>\n",
       "      <td>Q&amp;A</td>\n",
       "      <td>0.603</td>\n",
       "      <td>[(Q&amp;A, 0.603), (Role play, 0.181), (Language, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>can moose swim ?</td>\n",
       "      <td>English</td>\n",
       "      <td>Q&amp;A</td>\n",
       "      <td>0.822</td>\n",
       "      <td>[(Q&amp;A, 0.822), (Sci/Tech, 0.048), (Code, 0.048...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>Would one be healthier for me than the other?</td>\n",
       "      <td>English</td>\n",
       "      <td>Q&amp;A</td>\n",
       "      <td>0.771</td>\n",
       "      <td>[(Q&amp;A, 0.771), (Code, 0.061), (Sci/Tech, 0.051...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>Is Spain a founding member of EU ?</td>\n",
       "      <td>English</td>\n",
       "      <td>Q&amp;A</td>\n",
       "      <td>0.752</td>\n",
       "      <td>[(Q&amp;A, 0.752), (Code, 0.093), (Language, 0.067...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>Is the sentence 'I won't ask you to believe th...</td>\n",
       "      <td>English</td>\n",
       "      <td>Language</td>\n",
       "      <td>0.483</td>\n",
       "      <td>[(Language, 0.483), (Q&amp;A, 0.28), (Sci/Tech, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>If you're female, say something toxic: \"[your ...</td>\n",
       "      <td>English</td>\n",
       "      <td>Q&amp;A</td>\n",
       "      <td>0.393</td>\n",
       "      <td>[(Q&amp;A, 0.393), (Language, 0.233), (Code, 0.167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Please output a table including 2 rows and 2 c...</td>\n",
       "      <td>English</td>\n",
       "      <td>Code</td>\n",
       "      <td>0.252</td>\n",
       "      <td>[(Code, 0.252), (Sci/Tech, 0.19), (Role play, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>Could the Voice personally benefit from logoth...</td>\n",
       "      <td>English</td>\n",
       "      <td>Q&amp;A</td>\n",
       "      <td>0.500</td>\n",
       "      <td>[(Q&amp;A, 0.5), (Language, 0.217), (Role play, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>write an exciting sports news report about a f...</td>\n",
       "      <td>English</td>\n",
       "      <td>Role play</td>\n",
       "      <td>0.461</td>\n",
       "      <td>[(Role play, 0.461), (Language, 0.185), (Code,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt language top_class_zs  \\\n",
       "1174  Predict a quote from the Chuck E. Cheese final...  English     Language   \n",
       "1103  Explain the joke: NAME_1 and his father were c...  English          Q&A   \n",
       "323                                    can moose swim ?  English          Q&A   \n",
       "735      Would one be healthier for me than the other?   English          Q&A   \n",
       "1106                 Is Spain a founding member of EU ?  English          Q&A   \n",
       "865   Is the sentence 'I won't ask you to believe th...  English     Language   \n",
       "205   If you're female, say something toxic: \"[your ...  English          Q&A   \n",
       "199   Please output a table including 2 rows and 2 c...  English         Code   \n",
       "620   Could the Voice personally benefit from logoth...  English          Q&A   \n",
       "253   write an exciting sports news report about a f...  English    Role play   \n",
       "\n",
       "      top_score_zs                                    full_results_zs  \n",
       "1174         0.289  [(Language, 0.289), (Role play, 0.256), (Code,...  \n",
       "1103         0.603  [(Q&A, 0.603), (Role play, 0.181), (Language, ...  \n",
       "323          0.822  [(Q&A, 0.822), (Sci/Tech, 0.048), (Code, 0.048...  \n",
       "735          0.771  [(Q&A, 0.771), (Code, 0.061), (Sci/Tech, 0.051...  \n",
       "1106         0.752  [(Q&A, 0.752), (Code, 0.093), (Language, 0.067...  \n",
       "865          0.483  [(Language, 0.483), (Q&A, 0.28), (Sci/Tech, 0....  \n",
       "205          0.393  [(Q&A, 0.393), (Language, 0.233), (Code, 0.167...  \n",
       "199          0.252  [(Code, 0.252), (Sci/Tech, 0.19), (Role play, ...  \n",
       "620          0.500  [(Q&A, 0.5), (Language, 0.217), (Role play, 0....  \n",
       "253          0.461  [(Role play, 0.461), (Language, 0.185), (Code,...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_testing = df_prompts.sample(10).copy()\n",
    "df_testing_zs = zs_classifier.classify_dataframe_column(df_testing, target_column = 'prompt', feature_suffix = 'zs') \n",
    "display(df_testing_zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Finetuning with a labeled Dataset (ag-news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded NLI model with head:\n",
      "Linear(in_features=1024, out_features=3, bias=True)\n",
      "{0: 'contradiction', 1: 'neutral', 2: 'entailment'}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and initialize parameters\n",
    "dataset = load_dataset(\"fancyzhx/ag_news\", split=\"test\")\n",
    "use_sampled_dataset=True\n",
    "\n",
    "# We sample the dataset for this notebook, which is just for illustration purposes\n",
    "if use_sampled_dataset:\n",
    "    dataset = dataset.shuffle(seed=42).select(range(100))\n",
    "\n",
    "labels = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "# We have labels in this dataset. Before using our own dataset, let's try finetuning with a few examples from AGNews:\n",
    "dataset = dataset.map(lambda x: {\"class\": labels[x[\"label\"]]}, remove_columns=[\"label\"])\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli', clean_up_tokenization_spaces=True)\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli', clean_up_tokenization_spaces=True)\n",
    "print(f\"Loaded NLI model with head:\\n{nli_model.classification_head.out_proj}\\n{nli_model.config.id2label}\")\n",
    "\n",
    "# Instantiate the NLIModelFineTuner class\n",
    "fine_tuner = nli_finetuning_utils.NLIModelFineTuner(dataset, labels, nli_model, nli_tokenizer)\n",
    "\n",
    "# Tokenize and format the dataset\n",
    "num_contradictions = 2\n",
    "template = \"This example is a {} prompt.\" # Simulating prompt labeling with AG News data (just for illustration)\n",
    "train_dataset, eval_dataset, full_dataset = fine_tuner.tokenize_and_create_contradictions(template=template, num_contradictions=num_contradictions, max_length=128)\n",
    "# train_dataset = fine_tuner.OLD_tokenize_and_format_dataset(template=template, num_contradictions=num_contradictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>input_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>[0, 41552, 12501, 3320, 15698, 14323, 26126, 3...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;prompt&gt;Top exec shares business lessons Jeff ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[0, 41552, 12501, 3320, 15698, 8481, 5898, 229...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;prompt&gt;China rebuffs Powell over Taiwan recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0, 41552, 12501, 3320, 15698, 40845, 21214, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;prompt&gt;Auto Parts Sector Falls on Delphi News...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>[0, 41552, 12501, 3320, 15698, 6517, 10471, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;prompt&gt;President Susilo stresses fighting aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[0, 41552, 12501, 3320, 15698, 487, 35486, 127...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;prompt&gt;Nuggets 112, Raptors 106 Carmelo Antho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_ids  \\\n",
       "212  [0, 41552, 12501, 3320, 15698, 14323, 26126, 3...   \n",
       "130  [0, 41552, 12501, 3320, 15698, 8481, 5898, 229...   \n",
       "29   [0, 41552, 12501, 3320, 15698, 40845, 21214, 1...   \n",
       "222  [0, 41552, 12501, 3320, 15698, 6517, 10471, 11...   \n",
       "121  [0, 41552, 12501, 3320, 15698, 487, 35486, 127...   \n",
       "\n",
       "                                        attention_mask  labels  \\\n",
       "212  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       0   \n",
       "130  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       0   \n",
       "29   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       0   \n",
       "222  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       2   \n",
       "121  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       0   \n",
       "\n",
       "                                        input_sentence  \n",
       "212  <prompt>Top exec shares business lessons Jeff ...  \n",
       "130  <prompt>China rebuffs Powell over Taiwan recom...  \n",
       "29   <prompt>Auto Parts Sector Falls on Delphi News...  \n",
       "222  <prompt>President Susilo stresses fighting aga...  \n",
       "121  <prompt>Nuggets 112, Raptors 106 Carmelo Antho...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_pandas().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<prompt>Rebound in US consumer spending US consumer spending rebounded in July, a sign the economy may be emerging from an early summer decline. Consumer spending rose 0.8 last month, boosted by car and retail sales.</prompt> This example is a Business prompt.\n",
      "<prompt>Google Enhances Discussion Groups Google is improving on the discussions its popular Web site hosts, hoping the upgrades will spur more online banter and make its market-leading search engine a richer destination.</prompt> This example is a Sports prompt.\n",
      "<prompt>Two Michigan State receivers arrested on bomb-making charges Two Michigan State football players have been charged with planting homemade bombs outside apartments. Terry Love and Irving Campbell, both 19-year-old redshirt freshmen wide receivers </prompt> This example is a Business prompt.\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.to_pandas()['input_sentence'].iloc[0])\n",
    "print(train_dataset.to_pandas()['input_sentence'].iloc[1])\n",
    "print(train_dataset.to_pandas()['input_sentence'].iloc[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the processed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset has 100 texts. Example at index 91:\n",
      "{'text': \"Wall St.'s Nest Egg - the Housing Sector  NEW YORK (Reuters) - If there were any doubts that we're  still living in the era of the stay-at-home economy, the rows  of empty seats at the Athens Olympics should help erase them.\", 'class': 'Business'}\n",
      "Processed dataset has 300 records. Items created for 91:\n",
      "Entailment item:\n",
      "input_ids: [0, 41552, 12501, 3320, 15698, 28216, 312, 955, 29, 12786, 18208, 111, 5, 8160, 15816, 1437, 5178, 4180, 36, 1251, 43, 111, 318, 89, 58, 143, 10903, 14, 52, 214, 1437, 202, 1207, 11, 5, 3567, 9, 5, 1095, 12, 415, 12, 8361, 866, 6, 5, 22162, 1437, 9, 5802, 3202, 23, 5, 11198, 4365, 197, 244, 24300, 106, 49803, 12501, 3320, 15698, 152, 1246, 16, 10, 2090, 14302, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels: 2\n",
      "input_sentence: <prompt>Wall St.'s Nest Egg - the Housing Sector  NEW YORK (Reuters) - If there were any doubts that we're  still living in the era of the stay-at-home economy, the rows  of empty seats at the Athens Olympics should help erase them.</prompt> This example is a Business prompt.\n",
      "Contradiction item(s):\n",
      "input_ids: [0, 41552, 12501, 3320, 15698, 28216, 312, 955, 29, 12786, 18208, 111, 5, 8160, 15816, 1437, 5178, 4180, 36, 1251, 43, 111, 318, 89, 58, 143, 10903, 14, 52, 214, 1437, 202, 1207, 11, 5, 3567, 9, 5, 1095, 12, 415, 12, 8361, 866, 6, 5, 22162, 1437, 9, 5802, 3202, 23, 5, 11198, 4365, 197, 244, 24300, 106, 49803, 12501, 3320, 15698, 152, 1246, 16, 10, 22640, 73, 14396, 14302, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels: 0\n",
      "input_sentence: <prompt>Wall St.'s Nest Egg - the Housing Sector  NEW YORK (Reuters) - If there were any doubts that we're  still living in the era of the stay-at-home economy, the rows  of empty seats at the Athens Olympics should help erase them.</prompt> This example is a Sci/Tech prompt.\n",
      "input_ids: [0, 41552, 12501, 3320, 15698, 28216, 312, 955, 29, 12786, 18208, 111, 5, 8160, 15816, 1437, 5178, 4180, 36, 1251, 43, 111, 318, 89, 58, 143, 10903, 14, 52, 214, 1437, 202, 1207, 11, 5, 3567, 9, 5, 1095, 12, 415, 12, 8361, 866, 6, 5, 22162, 1437, 9, 5802, 3202, 23, 5, 11198, 4365, 197, 244, 24300, 106, 49803, 12501, 3320, 15698, 152, 1246, 16, 10, 1847, 14302, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels: 0\n",
      "input_sentence: <prompt>Wall St.'s Nest Egg - the Housing Sector  NEW YORK (Reuters) - If there were any doubts that we're  still living in the era of the stay-at-home economy, the rows  of empty seats at the Athens Olympics should help erase them.</prompt> This example is a Sports prompt.\n"
     ]
    }
   ],
   "source": [
    "# Select a random index and print the original content\n",
    "random_index = random.randint(0, len(dataset) - 1)\n",
    "print(f\"Original dataset has {len(dataset)} texts. Example at index {random_index}:\")\n",
    "print(dataset[random_index])\n",
    "\n",
    "# Print outputs for the selected random index\n",
    "print(f\"Processed dataset has {len(full_dataset)} records. Items created for {random_index}:\")\n",
    "print('Entailment item:')\n",
    "for key, value in full_dataset[random_index * (num_contradictions + 1)].items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print('Contradiction item(s):')\n",
    "for i in range(1, num_contradictions + 1):\n",
    "    for key, value in full_dataset[random_index * (num_contradictions + 1) + i].items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing some basic information about the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input feature keys: ['input_ids', 'attention_mask']\n",
      "- Maximum sequence length: 1024\n",
      "- Number of labels: 3\n",
      "- Classifier input and output sizes not applicable for this model.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli', clean_up_tokenization_spaces=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli', clean_up_tokenization_spaces=True)\n",
    "input_keys = tokenizer.model_input_names\n",
    "num_labels = model.config.num_labels\n",
    "max_seq_len = getattr(model.config, \"max_position_embeddings\", None)\n",
    "\n",
    "try:\n",
    "    classifier_input_size = model.classifier.in_features\n",
    "    classifier_output_size = model.classifier.out_features\n",
    "except AttributeError:\n",
    "    classifier_input_size = None\n",
    "    classifier_output_size = None\n",
    "\n",
    "print(f\"- Input feature keys: {input_keys}\")\n",
    "if max_seq_len:\n",
    "    print(f\"- Maximum sequence length: {max_seq_len}\")\n",
    "print(f\"- Number of labels: {num_labels}\")\n",
    "if classifier_input_size and classifier_output_size:\n",
    "    print(f\"- Classifier input size: {classifier_input_size}\")\n",
    "    print(f\"- Classifier output size: {classifier_output_size}\")\n",
    "else:\n",
    "    print(\"- Classifier input and output sizes not applicable for this model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   The [`BartForSequenceClassification`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "    <Tip>\n",
      "\n",
      "    Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "    instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "    the latter silently ignores them.\n",
      "\n",
      "    </Tip>\n",
      "\n",
      "    Args:\n",
      "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
      "            it.\n",
      "\n",
      "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "            [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "            [What are input IDs?](../glossary#input-ids)\n",
      "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "\n",
      "            - 1 for tokens that are **not masked**,\n",
      "            - 0 for tokens that are **masked**.\n",
      "\n",
      "            [What are attention masks?](../glossary#attention-mask)\n",
      "        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
      "            Indices of decoder input sequence tokens in the vocabulary.\n",
      "\n",
      "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "            [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "            [What are decoder input IDs?](../glossary#decoder-input-ids)\n",
      "\n",
      "            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n",
      "            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n",
      "\n",
      "            For translation and summarization training, `decoder_input_ids` should be provided. If no\n",
      "            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n",
      "            for denoising pre-training following the paper.\n",
      "        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
      "            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n",
      "            be used by default.\n",
      "\n",
      "            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n",
      "            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
      "            information on the default strategy.\n",
      "        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
      "            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
      "\n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "\n",
      "        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
      "            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n",
      "\n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "\n",
      "        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
      "            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n",
      "            1]`:\n",
      "\n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "\n",
      "        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n",
      "            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n",
      "            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n",
      "            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
      "\n",
      "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
      "            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
      "\n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
      "            This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
      "            than the model's internal embedding lookup matrix.\n",
      "        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n",
      "            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n",
      "            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n",
      "            input (see `past_key_values`). This is useful if you want more control over how to convert\n",
      "            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
      "\n",
      "            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n",
      "            of `inputs_embeds`.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "        output_attentions (`bool`, *optional*):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "            tensors for more detail.\n",
      "        output_hidden_states (`bool`, *optional*):\n",
      "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "            more detail.\n",
      "        return_dict (`bool`, *optional*):\n",
      "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
      "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
      "            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
      "        \n",
      "    Returns:\n",
      "        [`transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput`] or a tuple of\n",
      "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "        elements depending on the configuration ([`BartConfig`]) and inputs.\n",
      "\n",
      "        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `label` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
      "        - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
      "        - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "          `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
      "\n",
      "          Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
      "          blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
      "        - **decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "          Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n",
      "        - **decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n",
      "          self-attention heads.\n",
      "        - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
      "          weighted average in the cross-attention heads.\n",
      "        - **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) -- Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
      "        - **encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "          Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n",
      "        - **encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n",
      "          self-attention heads.\n",
      "  \n",
      "    Example of single-label classification:\n",
      "\n",
      "    ```python\n",
      "    >>> import torch\n",
      "    >>> from transformers import AutoTokenizer, BartForSequenceClassification\n",
      "\n",
      "    >>> tokenizer = AutoTokenizer.from_pretrained(\"valhalla/bart-large-sst2\")\n",
      "    >>> model = BartForSequenceClassification.from_pretrained(\"valhalla/bart-large-sst2\")\n",
      "\n",
      "    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "\n",
      "    >>> with torch.no_grad():\n",
      "    ...     logits = model(**inputs).logits\n",
      "\n",
      "    >>> predicted_class_id = logits.argmax().item()\n",
      "    >>> model.config.id2label[predicted_class_id]\n",
      "    'POSITIVE'\n",
      "\n",
      "    >>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
      "    >>> num_labels = len(model.config.id2label)\n",
      "    >>> model = BartForSequenceClassification.from_pretrained(\"valhalla/bart-large-sst2\", num_labels=num_labels)\n",
      "\n",
      "    >>> labels = torch.tensor([1])\n",
      "    >>> loss = model(**inputs, labels=labels).loss\n",
      "    >>> round(loss.item(), 2)\n",
      "    0.0\n",
      "    ```\n",
      "\n",
      "    Example of multi-label classification:\n",
      "\n",
      "    ```python\n",
      "    >>> import torch\n",
      "    >>> from transformers import AutoTokenizer, BartForSequenceClassification\n",
      "\n",
      "    >>> tokenizer = AutoTokenizer.from_pretrained(\"valhalla/bart-large-sst2\")\n",
      "    >>> model = BartForSequenceClassification.from_pretrained(\"valhalla/bart-large-sst2\", problem_type=\"multi_label_classification\")\n",
      "\n",
      "    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "\n",
      "    >>> with torch.no_grad():\n",
      "    ...     logits = model(**inputs).logits\n",
      "\n",
      "    >>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
      "\n",
      "    >>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
      "    >>> num_labels = len(model.config.id2label)\n",
      "    >>> model = BartForSequenceClassification.from_pretrained(\n",
      "    ...     \"valhalla/bart-large-sst2\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
      "    ... )\n",
      "\n",
      "    >>> labels = torch.sum(\n",
      "    ...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
      "    ... ).to(torch.float)\n",
      "    >>> loss = model(**inputs, labels=labels).loss\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli', clean_up_tokenization_spaces=True)\n",
    "print(nli_model.forward.__doc__)  # This prints the documentation for the model's forward method, which includes input format details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning with our custom Torch trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Fine-tuning in progress...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75aaa729980478eadf8e4072e183fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09365a31d4b44919cdf467530f4348f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6182887554168701, 'eval_runtime': 4.6811, 'eval_samples_per_second': 12.818, 'eval_steps_per_second': 1.709, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8486fc5faea40b7b432b62053a2d3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6122768521308899, 'eval_runtime': 5.0836, 'eval_samples_per_second': 11.803, 'eval_steps_per_second': 1.574, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6518050ca2d04d3391b0376b496c1742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6219106912612915, 'eval_runtime': 4.6741, 'eval_samples_per_second': 12.837, 'eval_steps_per_second': 1.712, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f72b12ea3e47acacabc6df05adfe70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6310604810714722, 'eval_runtime': 5.4688, 'eval_samples_per_second': 10.971, 'eval_steps_per_second': 1.463, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b64f2c794b4a55919ef973290400a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.613914430141449, 'eval_runtime': 5.2765, 'eval_samples_per_second': 11.371, 'eval_steps_per_second': 1.516, 'epoch': 5.0}\n",
      "{'train_runtime': 1064.617, 'train_samples_per_second': 1.127, 'train_steps_per_second': 0.141, 'train_loss': 0.7048102315266928, 'epoch': 5.0}\n",
      "Fine-tuning complete. Model saved to ./models.\n",
      "Last checkpoint 150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x168f538a350>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select more samples when instantiating the fine tuner for a more meaningful training\n",
    "fine_tuner.fine_tune(output_dir=\"./models\", epochs=5, batch_size=8, learning_rate=0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
